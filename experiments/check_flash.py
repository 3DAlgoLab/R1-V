import torch
print("FlashAttention available:", torch.backends.cuda.flash_sdp_enabled())



